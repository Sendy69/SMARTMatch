{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorda\\OneDrive\\Desktop\\MD5\\ML\\SMARTMatch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from mistralai import Mistral\n",
    "\n",
    "import base64\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "MODEL_EMBEDED = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretretment_file import clean_text,clean_and_format_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "Api_key= os.environ[\"mistral_jd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(image_path):\n",
    "    # api_url = 'https://api.mistral.ai/v1/chat/completions'\n",
    "\n",
    "    client= Mistral(api_key=Api_key)\n",
    "    base64_image = image_to_base64(image_path)\n",
    "\n",
    "    model= \"pixtral-12b-2409\"\n",
    "    messages= [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Tu es un comptable et tu dois extraire les informations d'une facture sous forme de photo en tenant compte du fait que les photos sont prise a des positions differente et souvent pas tres claire. Tu dois r√©pondre en JSON structur√©, All amounts must be floats with exactly two digits after the decimal point, using a dot as the decimal separator.Do not append any currency symbols to amounts.Any missing fields must be set to null. currency: Identify the currency used as an ISO code (EUR, USD, GBP, JPY, etc.) and not¬†as¬†a¬†symbol.All dates must be in the format yyyy-mm-dd\"\n",
    "                } \n",
    "            ],\n",
    "\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"\"\"D√©cris cette image sous forme de fichier JSON structur√© avec la cl√© : 'fields'.le format de date: YYYY-MM-DD et le format de l'heure: HH:MM:SS. Required fields:date: The payment due date,total_amount: The total amount due,\n",
    "                    vendor_name: name of the supply,currency: The current of the total amount, items: description: Description of the item or service,quantity: Quantity of the item,total_price: Total price for the item,tax:¬†Tax¬†amount, vendor_address: Address of the supplier,\n",
    "                    tax_rate: Tax rate, line_items: List of line items with description, quantity, unit price and total price. \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\", \n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.complete(\n",
    "        model=model, \n",
    "        messages=messages,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    try:\n",
    "        json_data = json.loads(content)\n",
    "        return json.dumps(json_data, indent=2)\n",
    "    except json.JSONDecodeError:\n",
    "        return json.dumps({\"error\": \"Invalid JSON response\", \"raw_response\": content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IMAGE_FOLDER=\"../dataset/receipts/\"\n",
    "# all_dataframes = []\n",
    "# def process_images(IMAGE_FOLDER):\n",
    "#     if not os.path.exists(IMAGE_FOLDER):\n",
    "#         raise ValueError(f\"Le dossier {IMAGE_FOLDER} n'existe pas.\")\n",
    "\n",
    "#     for filename_clean_clean_clean_clean in os.listdir(IMAGE_FOLDER):\n",
    "#         if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "#             image_path = os.path.join(IMAGE_FOLDER, filename)\n",
    "#             print(f\"üì∑ Traitement de : {filename}\")\n",
    "\n",
    "#             result = extract(image_path)\n",
    "#             try:\n",
    "#                 result = json.loads(result)\n",
    "#             except json.JSONDecodeError:\n",
    "#                 print(f\"‚ö†Ô∏è Erreur de d√©codage JSON pour {filename}\")\n",
    "#                 continue\n",
    "#             if \"error\" in result:\n",
    "#                 print(f\"‚ö†Ô∏è Erreur dans la r√©ponse pour {filename}: {result['error']}\")\n",
    "#                 continue\n",
    "#             if \"fields\" not in result:\n",
    "#                 print(f\"‚ö†Ô∏è Aucune donn√©e trouv√©e pour {filename}\")\n",
    "#                 continue\n",
    "#             if \"fields\" in result and isinstance(result[\"fields\"], list):\n",
    "#                 general_info = {field[\"name\"]: field[\"value\"] for field in result[\"fields\"] if field[\"name\"] != \"Items\"}\n",
    "#             if general_info:\n",
    "#                 df_general = pd.DataFrame([general_info])\n",
    "#                 df_general[\"filename\"] = image_path\n",
    "#                 all_dataframes.append(df_general)\n",
    "#             else:\n",
    "#                 print(f\"‚ö†Ô∏è Donn√©es invalides ou manquantes pour {filename}\")\n",
    "#                 all_dataframes.append(df_general)\n",
    "#             if isinstance(result, str) and \"Requests rate limit exceeded\" in result:\n",
    "#                 time.sleep(5)\n",
    "#                 continue\n",
    "#     if all_dataframes:\n",
    "#         final_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "#         return final_df\n",
    "#     else:\n",
    "#         print(\" Aucun fichier n'a √©t√© extrait avec succ√®s.\")\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(IMAGE_FOLDER):\n",
    "    if not os.path.exists(IMAGE_FOLDER):\n",
    "        raise ValueError(f\"Le dossier {IMAGE_FOLDER} n'existe pas.\")\n",
    "    \n",
    "    all_dataframes = []\n",
    "    \n",
    "    for filename in os.listdir(IMAGE_FOLDER):\n",
    "        if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            image_path = os.path.join(IMAGE_FOLDER, filename)\n",
    "            print(f\"üì∑ Traitement de : {filename}\")\n",
    "            \n",
    "            result = extract(image_path)\n",
    "            try:\n",
    "                result = json.loads(result)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Erreur de d√©codage JSON pour {filename}\")\n",
    "                continue\n",
    "            if not isinstance(result, dict):\n",
    "                print(f\"‚ö†Ô∏è R√©ponse inattendue pour {filename}, format incorrect.\")\n",
    "                continue\n",
    "            \n",
    "            if \"error\" in result:\n",
    "                print(f\"‚ö†Ô∏è Erreur dans la r√©ponse pour {filename}: {result['error']}\")\n",
    "                continue\n",
    "            \n",
    "            if \"fields\" not in result or not isinstance(result[\"fields\"], dict):\n",
    "                print(f\"‚ö†Ô∏è Aucune donn√©e valide trouv√©e pour {filename}\")\n",
    "                continue\n",
    "            \n",
    "            fields = result[\"fields\"]\n",
    "            general_info = {key: value for key, value in fields.items() if key != \"items\"}\n",
    "            \n",
    "            if general_info:\n",
    "                df_general = pd.DataFrame([general_info])\n",
    "                # df_general[\"date\"] = df_general[\"date\"].apply(clean_and_format_dates)\n",
    "                df_general[\"filename\"] = image_path\n",
    "                all_dataframes.append(df_general)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Donn√©es invalides ou manquantes pour {filename}\")\n",
    "                continue\n",
    "            \n",
    "            # if \"items\" in fields and isinstance(fields[\"items\"], list):\n",
    "            #     items_df = pd.DataFrame(fields[\"items\"])\n",
    "            #     items_df[\"filename\"] = image_path\n",
    "            #     all_dataframes.append(items_df)\n",
    "            \n",
    "            if isinstance(result, str) and \"Requests rate limit exceeded\" in result:\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "    \n",
    "    if all_dataframes:\n",
    "        final_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        final_df[\"date\"]= pd.to_datetime(final_df[\"date\"], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"Aucun fichier n'a √©t√© extrait avec succ√®s.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"fields\": {\n",
      "    \"date\": \"2014-05-22\",\n",
      "    \"total_amount\": 14.36,\n",
      "    \"vendor_name\": \"SmokeBox BBQ\",\n",
      "    \"currency\": \"USD\",\n",
      "    \"vendor_address\": \"2361 Whitney Ave, Hamden, CT 06518\",\n",
      "    \"tax_rate\": 0.06,\n",
      "    \"line_items\": [\n",
      "      {\n",
      "        \"description\": \"1 FXN DIET BIRCH\",\n",
      "        \"quantity\": 1,\n",
      "        \"unit_price\": 1.5,\n",
      "        \"total_price\": 1.5\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"1 FULL BOX\",\n",
      "        \"quantity\": 1,\n",
      "        \"unit_price\": 12.0,\n",
      "        \"total_price\": 12.0\n",
      "      }\n",
      "    ],\n",
      "    \"tax\": 0.86,\n",
      "    \"items\": [\n",
      "      {\n",
      "        \"description\": \"1 FXN DIET BIRCH\",\n",
      "        \"quantity\": 1,\n",
      "        \"total_price\": 1.5\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"1 FULL BOX\",\n",
      "        \"quantity\": 1,\n",
      "        \"total_price\": 12.0\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json1=extract(\"../dataset/receipts/1075-receipt.jpg\")\n",
    "print(json1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df= process_images()\n",
    "# if df is not None:\n",
    "#     df.head()\n",
    "# else:\n",
    "#     print(\" Aucune donn√©e extraite.\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bank_statement(bank_statement_path):\n",
    "    df = pd.read_csv(bank_statement_path)  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def match_releve_receipt(releve_df, receipt_df):\n",
    "    \n",
    "    date_tolerance_days = 2         \n",
    "    similarity_threshold = 0.75 \n",
    "    receipt_df['vendor_name_clean'] = receipt_df['vendor_name'].apply(clean_text)\n",
    "    releve_df['vendor_clean'] = releve_df['vendor'].apply(clean_text)\n",
    "\n",
    "    if 'supplier_emb' not in receipt_df.columns:\n",
    "        receipt_df = receipt_df.copy()\n",
    "        receipt_df['supplier_emb'] = receipt_df['vendor_name_clean'].apply(\n",
    "            lambda x: model.encode(x, convert_to_tensor=True)\n",
    "        )\n",
    "    \n",
    "    invoice_ids = []\n",
    "    similarity_percentages = []\n",
    "    \n",
    "    for idx, row in releve_df.iterrows():\n",
    "        montant = row['amount']\n",
    "        date_releve = pd.to_datetime(row['date'], errors='coerce')\n",
    "        vendor_text = row['vendor_clean']\n",
    "        \n",
    "        candidats = receipt_df[receipt_df['total_amount'] == montant].copy()\n",
    "        \n",
    "        if len(candidats) == 1:\n",
    "            best_match = candidats.iloc[0]\n",
    "            try:\n",
    "                invoice_id = best_match['filename']\n",
    "            except:\n",
    "                invoice_id = None\n",
    "            invoice_ids.append(invoice_id)\n",
    "            similarity_percentages.append(100.0)\n",
    "        \n",
    "        elif len(candidats) > 1:\n",
    "            candidats_date = candidats[\n",
    "                (candidats['date'] >= date_releve) &\n",
    "                (candidats['date'] <= date_releve + pd.Timedelta(days=date_tolerance_days))\n",
    "            ]\n",
    "            \n",
    "            if len(candidats_date) == 1:\n",
    "                best_match = candidats_date.iloc[0]\n",
    "                try:\n",
    "                    invoice_id = best_match['filename']\n",
    "                except:\n",
    "                    invoice_id = None\n",
    "                invoice_ids.append(invoice_id)\n",
    "                similarity_percentages.append(100.0)\n",
    "            \n",
    "            elif len(candidats_date) > 1:\n",
    "                vendor_emb = model.encode(vendor_text, convert_to_tensor=True)\n",
    "                candidats_date['similarity'] = candidats_date['supplier_emb'].apply(\n",
    "                    lambda emb: util.cos_sim(vendor_emb, emb).item()\n",
    "                )\n",
    "                max_similarity = candidats_date['similarity'].max()\n",
    "                best_candidate = candidats_date.loc[candidats_date['similarity'].idxmax()]\n",
    "                sim_percentage = max_similarity * 100\n",
    "                if max_similarity > similarity_threshold:\n",
    "                    try:\n",
    "                        invoice_id = best_candidate['filename']\n",
    "                    except:\n",
    "                        invoice_id = None\n",
    "                    invoice_ids.append(invoice_id)\n",
    "                    similarity_percentages.append(sim_percentage)\n",
    "                else:\n",
    "                    invoice_ids.append(None)\n",
    "                    similarity_percentages.append(sim_percentage)\n",
    "            else:\n",
    "                invoice_ids.append(None)\n",
    "                similarity_percentages.append(None)\n",
    "        else:\n",
    "            invoice_ids.append(None)\n",
    "            similarity_percentages.append(None)\n",
    "    \n",
    "    result_df = releve_df.copy()\n",
    "    result_df['invoice'] = invoice_ids\n",
    "    result_df['similarity'] = similarity_percentages\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m image_folder = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìÇ Entrez le chemin du dossier contenant les images : \u001b[39m\u001b[33m\"\u001b[39m).strip()\n\u001b[32m      3\u001b[39m releve = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìÇ Entrez le chemin du relev√© bancaire : \u001b[39m\u001b[33m\"\u001b[39m).strip()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m releve = \u001b[43mget_bank_statement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreleve\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m start_time = time.time()\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mget_bank_statement\u001b[39m\u001b[34m(bank_statement_path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_bank_statement\u001b[39m(bank_statement_path):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbank_statement_path\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorda\\OneDrive\\Desktop\\MD5\\ML\\SMARTMatch\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorda\\OneDrive\\Desktop\\MD5\\ML\\SMARTMatch\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorda\\OneDrive\\Desktop\\MD5\\ML\\SMARTMatch\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorda\\OneDrive\\Desktop\\MD5\\ML\\SMARTMatch\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorda\\OneDrive\\Desktop\\MD5\\ML\\SMARTMatch\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    image_folder = input(\"üìÇ Entrez le chemin du dossier contenant les images : \").strip()\n",
    "    releve = input(\"üìÇ Entrez le chemin du relev√© bancaire : \").strip()\n",
    "    releve = get_bank_statement(releve)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        df = process_images(image_folder)\n",
    "        if not df.empty:\n",
    "            print(\"\\nüéâ R√©sultats finaux:\")\n",
    "            # print(df.to_markdown(index=False))            \n",
    "            \n",
    "            # Sauvegarde des r√©sultats\n",
    "            timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            df.to_csv(f\"resultats_extraction_{timestamp}.csv\", index=False)\n",
    "            print(f\"\\nüíæ R√©sultats sauvegard√©s dans resultats_extraction_{timestamp}.csv\")\n",
    "            df_macht= match_releve_receipt(releve,df)\n",
    "            df_macht.to_csv(f\"resultats_matching_{timestamp}.csv\", index=False)\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è Aucun r√©sultat valide obtenu.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nüî• Erreur critique: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n‚è± Dur√©e totale: {time.time() - start_time:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_dataframes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mall_dataframes\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'all_dataframes' is not defined"
     ]
    }
   ],
   "source": [
    "all_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMARTMatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
